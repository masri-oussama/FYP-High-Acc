# -*- coding: utf-8 -*-
"""Bankruptcy perdiction 96% accuracy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZEx-ICNBM1q3Tslh54QSIb-V2Ck-WgS
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Read and Explore Data"""

# Read and Explore Data
df = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')

df.head()

df.shape

"""There are 96 features including Target variable. we obviously need to decompose the features for better prediction

lets check the null, na or any duplicate values in the dataset
"""

df.isnull().sum()

df.duplicated().sum()

"""There is no duplicate or null values in the data frame.
lets us check the datatype of the features
"""

df.info()

"""all 96 columns are either int or float value.

# Exploratory Data Analysis
Univariated data analysis
"""

# Commented out IPython magic to ensure Python compatibility.
#import required ploting libraries
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#let us check the Target variable
df['Bankrupt?'].value_counts()

sns.countplot(x=df['Bankrupt?'])
plt.title('Target feature - Bankrupt?')

"""There is imbalance in the Target value, we need to oversample it based

# Oversampling the Target value
"""

from imblearn.over_sampling import SMOTE

X=df.drop(labels=['Bankrupt?'], axis=1)
y=df['Bankrupt?']

"""Oversampling the Target vaiable using SMOTE"""

oversample = SMOTE()
X,y=oversample.fit_resample(X,y)

sns.countplot(x=y)

"""# Feature Selection"""

from sklearn.preprocessing import StandardScaler

"""Select 30 independent variables out of 96 variables."""

from sklearn.feature_selection import SelectFromModel, mutual_info_classif, f_classif, SelectKBest

feature_selection=SelectKBest(f_classif,k=30).fit(X,y)
#feat=feature_selection.fit(X_scale,y)

selected_features=X.columns[feature_selection.get_support()]

"""# Standardize the Independent Variable"""

scaler=StandardScaler()
X_scale=scaler.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scale, y,test_size=0.3)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
log_reg=LogisticRegression()
log_reg.fit(X_train,y_train)

log_reg.score(X_test,y_test)

y_pred=log_reg.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""# Metrics"""

accuracy_score(y_test,y_pred)

print(classification_report(y_test,y_pred))

"""# Support Vector Machine - Classification"""

from sklearn.svm import SVC

model = SVC()
model.fit(X_train, y_train)

model.score(X_test,y_test)

svc_predict=model.predict(X_test)

"""# Metrics"""

accuracy_score(y_test,svc_predict)

print(classification_report(y_test, svc_predict))

"""SVC model predicts the output with 96% accuracy where as Logit Regression predicts the output with 91% accuracy"""